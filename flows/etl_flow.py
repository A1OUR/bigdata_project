import os
import pandas as pd
from prefect import flow, task
from sqlalchemy import create_engine, text
import dask.dataframe as dd

# === TASK 1: EXTRACT ===
@task
def extract(csv_path: str = "/workspace/data/city_temperature2005.csv"):
    """
    Ð§Ð¸Ñ‚Ð°ÐµÑ‚ CSV Ð½Ð°Ð¿Ñ€ÑÐ¼ÑƒÑŽ Ð² Dask DataFrame â€” Ð±ÐµÐ· Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð² Ð¿Ð°Ð¼ÑÑ‚ÑŒ.
    """
    df = dd.read_csv(csv_path, assume_missing=True, dtype={"State": "object"})  # State Ð¼Ð¾Ð¶ÐµÑ‚ Ð±Ñ‹Ñ‚ÑŒ Ð¿ÑƒÑÑ‚Ñ‹Ð¼
    print(f"âœ… Extracted CSV as Dask DataFrame (npartitions={df.npartitions})")
    return df

# === TASK 2: LOAD RAW ===
@task
def load_raw(df):
    """
    Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ÑƒÐµÑ‚, Ð¿Ñ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÑ‚ Ð¸ Ð·Ð°Ð¿Ð¸ÑÑ‹Ð²Ð°ÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð² raw_weather Ð¿Ð°Ñ€Ñ‚Ð¸ÑÐ¼Ð¸.
    """
    # Ð¤Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ð¸Ñ Ð½ÐµÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ñ‹Ñ… Ð´Ð°Ñ‚
    df = df[
        (df["Day"] >= 1) &
        (df["Month"] >= 1) &
        (df["Month"] <= 12) &
        (df["Day"] <= 31)
    ]
    
    # Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð´Ð°Ñ‚Ñ‹
    df["date"] = dd.to_datetime(df[["Year", "Month", "Day"]], errors="coerce")
    df = df.dropna(subset=["date"])
    
    # Ð¢ÐµÐ¼Ð¿ÐµÑ€Ð°Ñ‚ÑƒÑ€Ð° Ð² Â°C
    df["avg_temp_c"] = (df["AvgTemperature"] - 32) * 5 / 9
    
    # Ð’Ñ‹Ð±Ð¾Ñ€ ÐºÐ¾Ð»Ð¾Ð½Ð¾Ðº
    df = df[["City", "Country", "date", "avg_temp_c"]].rename(
        columns={"City": "city", "Country": "country"}
    )
    
    # ÐŸÐ¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Ðº Ð‘Ð”
    db_host = os.getenv("DB_HOST", "localhost")
    db_url = f"postgresql://prefect:prefect@{db_host}:5432/climate_db"

    def write_partition(partition_df):
        if partition_df.empty:
            return 0
        worker_engine = create_engine(db_url)
        try:
            partition_df.to_sql(
                "raw_weather", 
                worker_engine, 
                if_exists="append", # Keep 'append' to respect existing table
                index=False, 
                chunksize=10000 
            )
        finally:
            worker_engine.dispose()
        return len(partition_df)

    print("ðŸš€ Starting parallel write to Postgres...")
    rows_per_partition = df.map_partitions(write_partition).compute()
    
    total_rows = sum(rows_per_partition)
    print(f"âœ… Loaded {total_rows} rows into raw_weather")
    return total_rows

# === TASK 3: TRANSFORM (Dask â†’ Ð°Ð³Ñ€ÐµÐ³Ð°Ñ†Ð¸Ñ) ===
@task
def transform_to_analytics():
    db_host = os.getenv("DB_HOST", "localhost")
    engine_url = f"postgresql://prefect:prefect@{db_host}:5432/climate_db"
    
    # Read using the numeric 'id' as the partition index
    df = dd.read_sql_table(
        "raw_weather",
        engine_url,
        index_col="id",
        npartitions=4
    )
    
    # Extract year from the date column
    df["year"] = dd.to_datetime(df["date"]).dt.year
    
    # Aggregate: Group by city and year
    result = df.groupby(["city", "year"])["avg_temp_c"].mean().reset_index()
    result = result.rename(columns={"avg_temp_c": "avg_annual_temp"})
    
    return result

@task
def load_analytics(df):
    db_host = os.getenv("DB_HOST", "localhost")
    engine = create_engine(f"postgresql://prefect:prefect@{db_host}:5432/climate_db")
    
    # Optional: Clear old analytics before appending new ones
    with engine.connect() as conn:
        conn.execute(text("TRUNCATE TABLE climate_analytics"))
        conn.commit()
    
    df_computed = df.compute()
    df_computed.to_sql("climate_analytics", engine, if_exists="append", index=False)
    
    print(f"âœ… Saved {len(df_computed)} rows to climate_analytics")
    return len(df_computed)

# === FLOW ===
@flow(name="Climate ETL Pipeline (Dask-Only)")
def climate_etl():
    df = extract()
    load_raw(df)
    analytics_df = transform_to_analytics()
    load_analytics(analytics_df)

if __name__ == "__main__":
    climate_etl()